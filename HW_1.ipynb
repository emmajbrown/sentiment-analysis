{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bymVftHXImRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "#from google.colab import files\n",
        "#from google.colab import drive\n",
        "\"\"\"uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/drive/My\\ Drive/foo.txt\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')\n",
        "\"\"\"\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove.6B.zip\n",
        "neg_words = []\n",
        "f = open(\"subjclueslen1-HLTEMNLP05.tff\",\"r\")\n",
        "f1 = f.readlines()\n",
        "words = [line.split() for line in f1]\n",
        "if words[5] == 'priorpolarity=negative':\n",
        "    neg_words.append(words[2][6:]) #good list of neg words\n",
        "#pos_words = []\n",
        "#f = open(\"subjclueslen1-HLTEMNLP05.tff\",\"r\")\n",
        "#f1 = f.readlines()\n",
        "#words = [line.split() for line in f1]\n",
        "#if words[5] == 'priorpolarity=positive':\n",
        "   #pos_words.append(words[2][6:]) #good list of pos words\n",
        "    '''\n",
        "array_of_pos_adjs = [] #['word1=great','word1=good'...]\n",
        "array_of_neg_adjs = [] #['word1=bad','word1=horrible'...]\n",
        "if words[3] == 'pos1=adj' and words[5] == 'priorpolarity=positive':\n",
        "    array_of_pos_adjs.append(words[2][6:])\n",
        "if words[3] == 'pos1=adj' and words[5] == 'priorpolarity=negative':\n",
        "  array_of_neg_adjs.append(words[2][6:])'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRARhMtI7AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the dumb features the model starts with.\n",
        "######################################################################\n",
        "\n",
        "#if the doc contains \"positive\" words, set the 'contains_pos_word key in feats = 1\n",
        "#if the doc contains \"negative\" words, set the 'contains_neg_word key in feats = 1\n",
        "#why are there only two keys in feats dictionary?\n",
        "\n",
        "def dumb_featurize(text):\n",
        "\tfeats = {}\n",
        "\twords = text.split(\" \")\n",
        "\n",
        "\tfor word in words:\n",
        "\t\tif word == \"love\" or word == \"like\" or word == \"best\":\n",
        "\t\t\tfeats[\"contains_positive_word\"] = 1\n",
        "\t\tif word == \"hate\" or word == \"dislike\" or word == \"worst\" or word == \"awful\":\n",
        "\t\t\tfeats[\"contains_negative_word\"] = 1\n",
        "\n",
        "\treturn feats\n",
        "\t#returns dictionary of 2 elements (not including bias). the 2 features of the 'text'\n",
        "\t#are \"contains_positive_word\" and \"contains_negative_word\". Check the 'text' to see\n",
        "\t#if 'text' has these features, if so, make them one.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGiM8qQiJOBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the sentiment classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class SentimentClassifier:\n",
        "\n",
        "\tdef __init__(self, feature_method):\n",
        "\t\tself.feature_vocab = {}\n",
        "\t\tself.feature_method = feature_method\n",
        "\n",
        "\n",
        "\t# Read data from file\n",
        "\tdef load_data(self, filename):\n",
        "\t\tdata = []\n",
        "\t\twith open(filename, encoding=\"utf8\") as file:\n",
        "\t\t\tfor line in file:\n",
        "\t\t\t\tcols = line.split(\"\\t\")\n",
        "\t\t\t\tlabel = cols[0]\n",
        "\t\t\t\ttext = cols[1].rstrip()\n",
        "\n",
        "\t\t\t\tdata.append((label, text))\n",
        "\t\treturn data\n",
        "\n",
        "\t# Featurize entire dataset\n",
        "\tdef featurize(self, data):\n",
        "\t\tfeaturized_data = []\n",
        "\t\tfor label, text in data:\n",
        "\t\t\tfeats = self.feature_method(text)\n",
        "\t\t\tfeaturized_data.append((label, feats))\n",
        "\t\treturn featurized_data\n",
        "\n",
        "\t# Read dataset and returned featurized representation as sparse matrix + label array\n",
        "\tdef process(self, dataFile, training = False):\n",
        "\t\tdata = self.load_data(dataFile)\n",
        "\t\tdata = self.featurize(data)\n",
        "\n",
        "\t\tif training:\t\t\t\n",
        "\t\t\tfid = 0\n",
        "\t\t\tfeature_doc_count = Counter()\n",
        "\t\t\tfor label, feats in data:\n",
        "\t\t\t\tfor feat in feats:\n",
        "\t\t\t\t\tfeature_doc_count[feat]+= 1\n",
        "\n",
        "\t\t\tfor feat in feature_doc_count:\n",
        "\t\t\t\tif feature_doc_count[feat] >= MIN_FEATURE_COUNT[self.feature_method.__name__]:\n",
        "\t\t\t\t\tself.feature_vocab[feat] = fid\n",
        "\t\t\t\t\tfid += 1\n",
        "\n",
        "\t\tF = len(self.feature_vocab)\n",
        "\t\tD = len(data)\n",
        "\t\tX = sparse.dok_matrix((D, F))\n",
        "\t\tY = np.zeros(D)\n",
        "\t\tfor idx, (label, feats) in enumerate(data):\n",
        "\t\t\tfor feat in feats:\n",
        "\t\t\t\tif feat in self.feature_vocab:\n",
        "\t\t\t\t\tX[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\t\t\tY[idx] = 1 if label == \"pos\" else 0\n",
        "\n",
        "\t\treturn X, Y\n",
        "\n",
        "\tdef load_test(self, dataFile):\n",
        "\t\tdata = self.load_data(dataFile)\n",
        "\t\tdata = self.featurize(data)\n",
        "\n",
        "\t\tF = len(self.feature_vocab)\n",
        "\t\tD = len(data)\n",
        "\t\tX = sparse.dok_matrix((D, F))\n",
        "\t\tY = np.zeros(D, dtype = int)\n",
        "\t\tfor idx, (data_id, feats) in enumerate(data):\n",
        "\t\t\t# print (data_id)\n",
        "\t\t\tfor feat in feats:\n",
        "\t\t\t\tif feat in self.feature_vocab:\n",
        "\t\t\t\t\tX[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\t\t\tY[idx] = data_id\n",
        "\n",
        "\t\treturn X, Y\n",
        "\n",
        "\t# Train model and evaluate on held-out data\n",
        "\tdef evaluate(self, trainX, trainY, devX, devY):\n",
        "\t\t(D,F) = trainX.shape\n",
        "\t\tself.log_reg = linear_model.LogisticRegression(C = L2_REGULARIZATION_STRENGTH[self.feature_method.__name__])\t\n",
        "\t\tself.log_reg.fit(trainX, trainY)\n",
        "\t\ttraining_accuracy = self.log_reg.score(trainX, trainY)\n",
        "\t\tdevelopment_accuracy = self.log_reg.score(devX, devY)\n",
        "\t\tprint(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\t\t\n",
        "\n",
        "\t# Predict labels for new data\n",
        "\tdef predict(self, testX, idsX):\n",
        "\t\tpredX = self.log_reg.predict(testX)\n",
        "\n",
        "\t\tout = open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\")\n",
        "\t\tout.write(\"Id,Expected\\n\")\n",
        "\t\tfor idx, data_id in enumerate(testX):\n",
        "\t\t\tout.write(\"%s,%s\\n\" % (idsX[idx], int(predX[idx])))\n",
        "\t\tout.close()\n",
        "\n",
        "\t# Write learned parameters to file\n",
        "\tdef printWeights(self):\n",
        "\t\tout = open(\"%s_%s\" % (self.feature_method.__name__, \"weights.txt\"), \"w\", encoding=\"utf8\")\n",
        "\t\treverseVocab = [None]*len(self.feature_vocab)\n",
        "\t\tfor feat in self.feature_vocab:\n",
        "\t\t\treverseVocab[self.feature_vocab[feat]] = feat\n",
        "\n",
        "\t\tout.write(\"%.5f\\t__BIAS__\\n\" % self.log_reg.intercept_)\n",
        "\t\tfor (weight, feat) in sorted(zip(self.log_reg.coef_[0], reverseVocab)):\n",
        "\t\t\tout.write(\"%.5f\\t%s\\n\" % (weight, feat))\n",
        "\t\tout.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnqjxd6fKPiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "##You may find it helpful to change these parameters to prevent the model from overfitting \n",
        "##and achieve higher performance\n",
        "######################################################################\n",
        "\n",
        "# regularization strength to control overfitting (values closer to 0  = stronger regularization)\n",
        "L2_REGULARIZATION_STRENGTH = {\"dumb_featurize\": 1, \"fancy_featurize\": .15 }\n",
        "#5,5, .2\n",
        "# must observe feature at least this many times in training data to include in model\n",
        "MIN_FEATURE_COUNT = {\"dumb_featurize\": 10,  \"fancy_featurize\": 6 }\n",
        "#5,5, 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxKmEqI5JY71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fancy_featurize(text):\n",
        "  features = {}\n",
        "  text = text.lower() #string\n",
        "  text = stop_words(text) #string\n",
        "  features['sentence_length'] = sentence_length(text)\n",
        "  text = re.sub('[^\\w]+', \" \", text) #get rid of punctuation, return string\n",
        "  features.update(bag_of_words(text)) #returns dict\n",
        "  features.update(tuple_bag_of_words(bigram(text)))\n",
        "  features.update(neg_density(text.split(\" \"),features,neg_words))\n",
        "  return features\n",
        "\n",
        "def sentence_length(text):\n",
        "  sent = re.split('\\.|\\?|\\!',text)\n",
        "  count_sent = len(sent)\n",
        "  count_words = 0\n",
        "  for s in sent:\n",
        "    count_words += len(s.split(\" \"))\n",
        "  return count_words/count_sent\n",
        "\n",
        "def neg_density(word_list,features,neg_words):\n",
        "  count_neg = 0\n",
        "  count = Counter(word_list)\n",
        "  for k,v in count.items():\n",
        "    #k = k.rstrip()\n",
        "    if k in neg_words:\n",
        "      count_neg += v\n",
        "  count_total = len(word_list)\n",
        "  neg_density = count_neg/count_total\n",
        "  features['neg_density'] = neg_density\n",
        "  return features\n",
        "\n",
        "def bigram(text): #returns list of tuples\n",
        "  bigrams_list = []\n",
        "  words = text.split(\" \")\n",
        "  for i in range(len(words)-1):\n",
        "    bigrams_list.append((words[i], words[i+1]))\n",
        "  return bigrams_list\n",
        "\n",
        "def stop_words(text): \n",
        "  stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "  #stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "  #filtered_words = [w for w in words if w not in stop_words]\n",
        "  words = text.split(\" \")\n",
        "  filter_words = \"\"\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      filter_words += w + \" \"\n",
        "  return filter_words #return string\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B13_8L-Do0B2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def tuple_bag_of_words(words):\n",
        "  word_bag = {}  \n",
        "  count = Counter(words) #count # times each tuple appears\n",
        "  count = dict(count)\n",
        "  for word in words:\n",
        "    word_bag[str(word)] = count[word]\n",
        "  return word_bag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPqUozF89TZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words(words):\n",
        "  word_bag = {}\n",
        "  words = words.split(\" \")\n",
        "  count = Counter(words) #count # times each tuple appears\n",
        "  count = dict(count)\n",
        "  for word in words:\n",
        "    word_bag[str(word)] = count[word]\n",
        "  return word_bag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1KDeWiOobc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this made it much worse\n",
        "def count_caps(count_upper,features):\n",
        "  features['count_caps'] = count_upper\n",
        "  return features\n",
        "\n",
        "def pos_density(word_list,features,pos_words):\n",
        "  count_pos = 0\n",
        "  count = Counter(word_list)\n",
        "  for k,v in count.items():\n",
        "    #k = k.rstrip()\n",
        "    if k in pos_words:\n",
        "      count_pos += v\n",
        "  count_total = len(word_list)\n",
        "  pos_density = count_pos/count_total\n",
        "  features['pos_density'] = pos_density\n",
        "  return features\n",
        "\n",
        "#made it worse\n",
        "def trigram(words,features):\n",
        "  trigrams_list = []\n",
        "  for i in range(len(words)-2):\n",
        "    trigrams_list.append((words[i], words[i+1],words[i+2]))\n",
        "  #bigrams = [(x, i.split()[j + 1]) for i in words for j, x in enumerate(i.split()) if j < len(i.split()) - 1] \n",
        "  #bigrams = str(zip(*[words[i:] for i in range(2)]))\n",
        "  features.update(bag_of_words(trigrams_list))\n",
        "  return features\n",
        "\n",
        "def question(words,features):\n",
        "  count_question = 0\n",
        "  for w in words:\n",
        "    if w == '\\?':\n",
        "      count_question += 1\n",
        "  #match = re.search('\\?', text)\n",
        "  #if match:\n",
        "    #features['question'] = 1\n",
        "  #else:\n",
        "    #features['question'] = 0\n",
        "  features['question_num'] = count_question\n",
        "  return features\n",
        "\n",
        "#this function doesn't do anything\n",
        "\n",
        "\n",
        "def caps(text,features):\n",
        "  match = re.search('[A-Z\\s]+',text)\n",
        "  if match:\n",
        "    features['all_caps'] = 1\n",
        "  else:\n",
        "    features['all_caps'] = 0\n",
        "  return features\n",
        "\n",
        "def count_adjs(text,features):\n",
        "  count_pos_adjs = 0\n",
        "  count_neg_adjs = 0\n",
        "  #f = open(\"subjclueslen1-HLTEMNLP05.tff\",\"r\")\n",
        "  #f1 = f.readlines()\n",
        "  #words = [line.split() for line in f1]\n",
        "  word_list = text.split(\" \")\n",
        "  count = Counter(word_list)\n",
        "  for k,v in count.items():\n",
        "    #k = k.rstrip()\n",
        "    if k in array_of_pos_adjs:\n",
        "      count_pos_adjs += v\n",
        "    if k in array_of_neg_adjs:\n",
        "      count_neg_adjs += v\n",
        "  \n",
        "  features['count_pos_adjs'] = count_pos_adjs\n",
        "  features['count_neg_adjs'] = count_neg_adjs\n",
        "  return features\n",
        "\n",
        "#this feature was not that effective -- probably equal number of '..' in pos + neg\n",
        "#didn't affect anything\n",
        "def dot(words,features):\n",
        "  count_dot_dot = 0\n",
        "  for w in words:\n",
        "    if w == '\\.\\.' or w == '\\.\\.\\.':\n",
        "      count_dot_dot += 1\n",
        "  '''\n",
        "  match1 = re.search('\\.\\.', text)\n",
        "  match2 = re.search('\\.\\.\\.',text)\n",
        "  if match1 or match2:\n",
        "    features['dot_dot'] = 1\n",
        "  else:\n",
        "    features['dot_dot'] = 0\n",
        "  '''\n",
        "  features['dot_dot'] = count_dot_dot\n",
        "  return features\n",
        "\n",
        "\n",
        "\n",
        "def create_pos_neg(word_list,features):\n",
        "  f = open(\"subjclueslen1-HLTEMNLP05.tff\",\"r\")\n",
        "  f1 = f.readlines()\n",
        "  words = [line.split() for line in f1]\n",
        "  if words[5] == 'priorpolarity=positive':\n",
        "    #pos_words.append(words[2][6:]) \n",
        "    features['positive_word'] = 1\n",
        "  if words[5] == 'priorpolarity=negative':\n",
        "    features['negative_word'] = 1\n",
        "  return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#this didn't make any changes to the dev accuracy\n",
        "def feat1(words,features):\n",
        "  c = [tuple(words[i:i+2]) for i in range(len(words)-2)]\n",
        "  for elem in c:\n",
        "    if elem == ('bad', 'movie') or elem == ('worst','movie'):\n",
        "      features['bad/worst_movie'] = 1\n",
        "    if elem == ('good', 'movie') or elem == ('great','movie'):\n",
        "      features['good/great_movie'] = 1\n",
        "    else:\n",
        "      features['good/great_movie'] = 0\n",
        "      features['bad/worst_movie'] = 0\n",
        "  return features\n",
        "\n",
        "def feat3(text,features):\n",
        "  features['char_count'] = len(text) #made it guess more incorrectly\n",
        "  return features\n",
        "  #features['punctuation_count'] = len(\"\".join(i for i in text if i in string.punctuation))\n",
        "\n",
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embedded_dict.keys(), key=lambda word: numpy.linalg.norm((embedded_dict[word]- embedding)))\n",
        "\n",
        "#makes it worse...\n",
        "def punc(text,features):\n",
        "  count_punc = 0\n",
        "  count = Counter(text.split())\n",
        "\n",
        "  for k,v in count.items():\n",
        "    if k in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~':\n",
        "      count_punc += 1\n",
        "  features['punctuation_count'] = count_punc\n",
        "  \n",
        "  return features\n",
        "\n",
        "def question(words,features):\n",
        "  count_question = 0\n",
        "  for w in words:\n",
        "    if w == '\\?':\n",
        "      count_question += 1\n",
        "  #match = re.search('\\?', text)\n",
        "  #if match:\n",
        "    #features['question'] = 1\n",
        "  #else:\n",
        "    #features['question'] = 0\n",
        "  features['question_num'] = count_question\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT7xD4k2QiCH",
        "colab_type": "code",
        "outputId": "c447d91e-adb8-4897-a7eb-4c96008e8798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "#This code gets the train/dev/test files from github and imports them into Colab\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/test.txt.zip\n",
        "!unzip test.txt.zip"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-04 06:19:45--  https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1427184 (1.4M) [text/plain]\n",
            "Saving to: ‘train.txt.7’\n",
            "\n",
            "\rtrain.txt.7           0%[                    ]       0  --.-KB/s               \rtrain.txt.7         100%[===================>]   1.36M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-02-04 06:19:45 (11.6 MB/s) - ‘train.txt.7’ saved [1427184/1427184]\n",
            "\n",
            "--2020-02-04 06:19:46--  https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1474560 (1.4M) [text/plain]\n",
            "Saving to: ‘dev.txt.7’\n",
            "\n",
            "dev.txt.7           100%[===================>]   1.41M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-02-04 06:19:46 (16.1 MB/s) - ‘dev.txt.7’ saved [1474560/1474560]\n",
            "\n",
            "--2020-02-04 06:19:47--  https://raw.githubusercontent.com/dbamman/nlp20/master/HW_1/test.txt.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13100860 (12M) [application/zip]\n",
            "Saving to: ‘test.txt.zip.7’\n",
            "\n",
            "test.txt.zip.7      100%[===================>]  12.49M  39.6MB/s    in 0.3s    \n",
            "\n",
            "2020-02-04 06:19:47 (39.6 MB/s) - ‘test.txt.zip.7’ saved [13100860/13100860]\n",
            "\n",
            "Archive:  test.txt.zip\n",
            "replace test.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.txt                \n",
            "replace __MACOSX/._test.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._test.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgwgKmYWLlc8",
        "colab_type": "code",
        "outputId": "ae4ebc1a-33ac-4062-8ef9-5d660951439b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#This cell trains two models: one on the dumb features and one on your fancy\n",
        "#features.  It will store the test set predictions in a csv.\n",
        "#The weights will be stored in a text file. \n",
        "#To access the files, click on the folder icon in the left sidebar.\n",
        "#You can preview the files in Colab by double clicking or download the files by \n",
        "#right clicking and selecting Download.\n",
        "if __name__ == \"__main__\":\n",
        "  trainingFile = \"./train.txt\"\n",
        "  evaluationFile = \"./dev.txt\"\n",
        "  testFile = \"./test.txt\"\n",
        "\n",
        "  for feature_method in [dumb_featurize, fancy_featurize]:\n",
        "    sentiment_classifier = SentimentClassifier(feature_method)\n",
        "    trainX, trainY = sentiment_classifier.process(trainingFile, training=True)\n",
        "    devX, devY = sentiment_classifier.process(evaluationFile, training=False)\n",
        "    testX, idsX = sentiment_classifier.load_test(testFile)\n",
        "    sentiment_classifier.evaluate(trainX, trainY, devX, devY)\n",
        "    sentiment_classifier.printWeights()\n",
        "    sentiment_classifier.predict(testX, idsX)\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: dumb_featurize, Features: 2, Train accuracy: 0.604, Dev accuracy: 0.611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Method: fancy_featurize, Features: 3933, Train accuracy: 0.999, Dev accuracy: 0.828\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}